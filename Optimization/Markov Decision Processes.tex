\documentclass[12pt]{amsart}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts,setspace}
\usepackage[shortlabels]{enumitem}
\usepackage{exercise, chngcntr}
\usepackage{cite}


\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}{Conjecture}
\newtheorem{defn}[thm]{Definition}
\newtheorem{note}[thm]{Note}
\newtheorem{ex}[thm]{Exercise}
\newtheorem{exm}[thm]{Example}
\newtheorem{sol}[thm]{Solution}


\newcommand{\al}{\alpha}
\newcommand{\Gam}{\Gamma}
\newcommand{\gam}{\gamma}
\newcommand{\be}{\beta} 
\newcommand{\del}{\delta} 
\newcommand{\Del}{\Delta}
\newcommand{\lam}{\lambda}  
\newcommand{\Lam}{\Lambda} 
\newcommand{\ep}{\epsilon}
\newcommand{\sig}{\sigma} 
\newcommand{\om}{\omega}
\newcommand{\Om}{\Omega}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}

\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\MA}{\mathcal{A}}
\newcommand{\MB}{\mathcal{B}}
\newcommand{\MF}{\mathcal{F}}
\newcommand{\MG}{\mathcal{G}}
\newcommand{\ML}{\mathcal{L}}
\newcommand{\MN}{\mathcal{N}}
\newcommand{\MS}{\mathcal{S}}
\newcommand{\MP}{\mathcal{P}}
\newcommand{\ME}{\mathcal{E}}
\newcommand{\MT}{\mathcal{T}}
\newcommand{\MM}{\mathcal{M}}
\newcommand{\MW}{\mathcal{W}}

\newcommand{\RG}{[0,\infty]}
\newcommand{\Rg}{[0,\infty)}
\newcommand{\limfn}{\liminf \limits_{n \rightarrow \infty}}
\newcommand{\limpn}{\limsup \limits_{n \rightarrow \infty}}
\newcommand{\limn}{\lim \limits_{n \rightarrow \infty}}
\newcommand{\convt}[1]{\xrightarrow{\text{#1}}}
\newcommand{\conv}[1]{\xrightarrow{#1}} 

\newcommand{\Ll}{L^1_{\text{loc}}(\R^n)}
\newcommand{\seq}[1]{(x_{#1})_{#1 \in \N}}

\newcommand{\n}{\Vert}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\E}{\mathbb{E}}
 
\begin{document}

\title{Markov Decision Process Notes}
\author[James]{Carson James}
\maketitle


\tableofcontents

\section{Introduction}

\subsection{Setup}
\begin{defn}
Let $\MS$, $\MA$ be finite sets, $(S_t)_{t=0}^{\infty}$ a sequence of $S$-valued random variables. For each $s \in \MS$, let $\MA_s \subset \MA$. For each $s \in \MS$ and $a \in \MA_s$, let $P^a_s: \MS \rightarrow [0,1]$ given by $P^a_s(s') \mapsto P^{a}_{s,s'}$ define a probabilty measure on $\MS$, that is: $$\sum_{s' \in \MS} P^{a}_{s,s'} = 1$$ Suppose that $(S_t)_{t=0}^{\infty}$ satisfies the following markov property: For each $t \in \N_0$, $s', s, s_{t-1}, \cdots, s_0 \in \MS$ and $a \in \MA_s$,
\begin{align*}
\P(S_{t+1} 
&= s'| S_t =s, S_{t-1} = s_{t-1}, \cdots, S_0=s_0) \\
&= \P(S_{t+1} = s'| S_t =s) \\
&= P^a_{s, s'}
\end{align*}

For each $t \in \N_0$, let $v_t: \MS \times \MA \times \MS  \rightarrow \R$ and $\pi_t:\MS \rightarrow \MA$ where for each $s \in \MS$, $\pi_t(s) \in \MA_s$. Define $\pi = (\pi_t)_{t=0}^{\infty}$. Then the tuple $(\MS, \MA, P, v, \pi)$ is said to be a \textbf{Markov decision process} or MDP with states $\MS$, actions $\MA$, transition probabilites $P^a_{s,s'}$, values $(v_t)_{t=0}^{\infty}$ and policy $\pi$.
\end{defn}

\begin{note}
The values $(v_t)_{t=0}^{\infty}$ are typically called the ``reward" or ``cost" values which has to do with whether the goal is to maximize or minimize total cost. These notes maintain generality and will simply refer to $(v_t)_{t=0}^{\infty}$ as the values. We do the same with the total cost function defined below.
\end{note}
\newpage
\section{Finite Horizon}

\subsection{Values}
\begin{note}
In the following section, we will consider a MDP over the time horizon of $t=0,1, \cdots, n$. The time $t = n$ is terminal and therefore no action will be chosen at this time. Therefore we may assume that the value at this time will only depend on the state and we denote this value by $v_n:\MS \rightarrow \R$. In addition, we may assume that $\pi = (\pi_t)_{t=0}^{n-1}$.
\end{note}
\begin{defn} 
Let $\pi$ be a policy. We define the \textbf{expected total value function} for the policy $\pi$, $V^{\pi}: \MS \rightarrow \R$, by $$V^{\pi}(s) = \E_{\pi}\bigg[\sum_{t =0}^{n-1} v_t(S_t, \pi_t(S_t), S_{t+1})+ v_n(S_n)|S_0=s\bigg]$$

and we define the \textbf{expected total discounted value function} for the policy $\pi$ by $$V^{\pi}_\gam (s) = \E_{\pi}\bigg[\sum_{t =0}^{n-1} \gam^t v_t(S_t, \pi_t(S_t), S_{t+1}) + \gam^n v_n(S_n)|S_0=s\bigg]$$
\end{defn}

\begin{thm}{\textbf{The Bellman Equation}:}
Define 
\[ V_t^{\pi}(s) = 
\begin{cases}
v_n(s) & t = n \\
\sum\limits_{s' \in \MS} P^{\pi_t(s)}_{s, s'}\bigg[v_t(s, \pi_t(s), s') + V_{t+1}(s')\bigg] & 0 \leq t < n
\end{cases}
\]

and define 

\[ V_{\gam, t}^{\pi}(s) = 
\begin{cases}
v_n(s) & t = n \\
\sum\limits_{s' \in \MS} P^{\pi_t(s)}_{s, s'}\bigg[v_t(s, \pi_t(s), s') + \gam V_{t+1}(s')\bigg] & 0 \leq t < n
\end{cases}
\]

Then $V^{\pi}(s) = V_0^{\pi}(s)$ and $V_\gam ^{\pi}(s) = V_{\gam, 0}^{\pi}(s)$.
\end{thm}

\subsection{Optimization}

\begin{defn}
Let $\Pi$ be the set of all policies and $\pi^* \in \Pi$. Then $\pi^*$ is said to be a \textbf{maximal policy} if$$V^{\pi^*}(s) = \sup_{\pi \in \Pi}V^{\pi}(s)$$ If we are discounting, then $\pi^*$ is said to be a \textbf{maximal discounted policy} if $$V_{\gam}^{\pi^*}(s) = \sup_{\pi \in \Pi}V_{\gam}^{\pi}(s)$$ 
\end{defn}

\begin{note}
We can define a ``minimal policy" and a ``minimal discounted policy" similarly.
\end{note}
\newpage

\begin{thm}{\textbf{Bellman Optimality Equation}} 
\begin{enumerate}
\item If $\pi^*$ is a maximal policy, then 
\[ V_t^{\pi^*}(s) =
\begin{cases}
 v_n(s) & t = n \\
 \max\limits_{a \in \MA} \sum\limits_{s' \in \MS}P^{a}_{s, s'} \bigg[v_t(s,a, s') + V_{t+1}^{\pi^*}(s')\bigg] & t= 0, 1, \cdots, n-1
\end{cases}
\]

and $$\pi^*_t(s) = \argmax_{a \in \MA} \sum\limits_{s' \in \MS}P^{a}_{s, s'} \bigg[v_t(s, a, s') +  V_{t+1}^{\pi^*}(s')\bigg]$$ 

\item If $\pi^*$ is a maximal discounted policy, then 
\[ V_{\gam, t}^{\pi^*}(s) =
\begin{cases}
 v_n(s) & t = n \\
 \max\limits_{a \in \MA} \sum\limits_{s' \in \MS}P^{a}_{s, s'} \bigg[v_t(s,a, s') + \gam V_{t+1}^{\pi^*}(s')\bigg] & t= 0, 1, \cdots, n-1
\end{cases}
\]

and $$\pi^*_t(s) = \argmax_{a \in \MA} \sum\limits_{s' \in \MS}P^{a}_{s, s'} \bigg[v_t(s, a, s') +  \gam V_{t+1}^{\pi^*}(s')\bigg]$$ 
\end{enumerate}
\end{thm}

\begin{note}
A similar result holds for minimal policies and minimal discounted policies.
\end{note}

\begin{defn}
The process of computing $\pi^*$ via the Bellman optimality equation is known as \textbf{value iteration}. It is a dynamic programming algorithm. 
\end{defn}

\subsection{Examples}

\begin{exm}{\textbf{Inventory Control:}} 
A company is moving product down the supply chain over the times $t=0, 1, 2, 3$. At the beginning of each time interval $t = 0, 1, 2$ the company 

\begin{enumerate}
\item has an inventory $s$ between $-2$ to $2$ units of product (negative in the case of a backlog of customer orders) 
\item orders (and immediately receives) $a$ units of product based on the inventory $s$
\item sells product to customers according to a random demand $d$ experienced during time $t$. The maximum demand at time $t$ is $2$ units of product.
\end{enumerate}
The demand at time $t$ is given by
\begin{align*}
&P(d=0) = .1\\
&P(d=1) = .6\\
&P(d=2) = .3
\end{align*} 
The cost to
\begin{enumerate}
\item  order one unit of product is 1 .
\item  hold one unit of product after demand is 2.
\item  hold one unit of backlogged product is 3
\end{enumerate}

Suppose that the terminal cost for each possible amount of inventory is $0$. Find the minimal expected total cost given that we start with no inventory and a minimal policy for ordering inventory.
\end{exm}

\begin{sol}
We have $\MS= \{-2, -2, 0, 1, 2\}$, $\MA_s = \{0, 1, \cdots,  2-s\}$, $v_t(s, a, s') = a + 2\max(0, s')+3\max(0,-s')$ and $v_3(s) = 0$ . The transition probabilites, organized by action, are given by:

\begin{align*}
&P^0_{-2,-2} = 1 ,P^0_{-2,-1} = 0, P^0_{-2,0} = 0, P^0_{-2,1} = 0, P^0_{-2,2} = 0\\
&P^0_{-1,-2} = .9, P^0_{-1,-1} = .1, P^0_{-1,0} = 0, P^0_{-1,1} = 0, P^0_{-1, 2} = 0\\ 
&P^0_{0,-2} = .3 ,P^0_{0,-1} = .6, P^0_{0,0} = .1, P^0_{0,1} = 0, P^0_{0,2} = 0\\
&P^0_{1,-2} = 0, P^0_{1,-1} = .3, P^0_{1,0} = .6, P^0_{1,1} = .1, P^0_{1, 2} = 0\\ 
&P^0_{2,-2} = 0 ,P^0_{2,-1} = 0, P^0_{2,0} = .3, P^0_{2,1} = .6, P^0_{2,2} = .1\\
\end{align*}

\begin{align*}
&P^1_{-2,-2} = .9 ,P^1_{-2,-1} = .1, P^1_{-2,0} = 0, P^1_{-2,1} = 0, P^1_{-2,2} = 0\\
&P^1_{-1,-2} = .3, P^1_{-1,-1} = .6, P^1_{-1,0} = .1, P^1_{-1,1} = 0, P^1_{-1, 2} = 0\\ 
&P^1_{0,-2} = 0 ,P^1_{0,-1} = .3, P^1_{0,0} = .6, P^1_{0,1} = .1, P^1_{0,2} = 0\\
&P^1_{1,-2} = 0, P^1_{1,-1} = 0, P^1_{1,0} = .3, P^1_{1,1} = .6, P^1_{1, 2} = .1\\ 
\end{align*}

\begin{align*}
&P^2_{-2,-2} = .3 ,P^2_{-2,-1} = .6, P^2_{-2,0} = .1, P^2_{-2,1} = 0, P^2_{-2,2} = 0\\
&P^2_{-1,-2} = 0, P^2_{-1,-1} = .3, P^2_{-1,0} = .6, P^2_{-1,1} = .1, P^2_{-1, 2} = 0\\ 
&P^2_{0,-2} = 0 ,P^2_{0,-1} = 0, P^2_{0,0} = .3, P^2_{0,1} = .6, P^2_{0,2} = .1\\ 
\end{align*}

\begin{align*}
&P^3_{-2,-2} = 0, P^3_{-2,-1} = .3, P^3_{-2,0} = .6, P^3_{-2,1} = .1, P^3_{-2,2} = 0\\
&P^3_{-1,-2} = 0, P^3_{-1,-1} = 0, P^3_{-1,0} = .3, P^3_{-1,1} = .6, P^3_{-1, 2} = .1\\ 
\end{align*}

\begin{align*}
&P^4_{-2,-2} = 0, P^4_{-2,-1} = .3, P^4_{-2,0} = .6, P^4_{-2,1} = .1, P^4_{-2,2} = 0 
\end{align*}

Since our initial inventory is $0$, we wish to find the minimal policy $\pi^*$ and $V^{\pi^*}_0(0)$. Now, using the bellman optimality equation, we know that the maximal policy $$\pi^*_2(s) = \argmin_{a \in \MA_s} \sum_{s' \in \MS} P^a_{s,s'} \bigg[ a + 2\max(0, s')+3\max(0,-s')\bigg]$$ and $$ V^{\pi^*}_{2}(s) = \min_{a \in \MA_s} \sum_{s' \in \MS} P^a_{s,s'} \bigg[ a + 2\max(0, s')+3\max(0,-s')\bigg]$$
\begin{align*}
j
\end{align*}
\end{sol}

\end{document}