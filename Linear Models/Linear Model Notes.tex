\documentclass[12pt]{amsart}
 \usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts,setspace}
\usepackage[shortlabels]{enumitem}
\usepackage{exercise, chngcntr}
\usepackage{physics}
\usepackage{mathtools}
%
%
%
\newif\ifhideproofs
%\hideproofstrue %uncomment to hide proofs
%
%
%
%
\ifhideproofs
\usepackage{environ}
\NewEnviron{hide}{}
\let\proof\hide
\let\endproof\endhide
\fi



\newcommand\Item[1][]{%
  \ifx\relax#1\relax  \item \else \item[#1] \fi
  \abovedisplayskip=0pt\abovedisplayshortskip=0pt~\vspace*{-\baselineskip}}



\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}{Conjecture}
\newtheorem{defn}[thm]{Definition}
\newtheorem{note}[thm]{Note}
\newtheorem{ex}[thm]{Exercise}


\newcommand{\al}{\alpha}
\newcommand{\Gam}{\Gamma}
\newcommand{\be}{\beta} 
\newcommand{\del}{\delta} 
\newcommand{\Del}{\Delta}
\newcommand{\lam}{\lambda}  
\newcommand{\Lam}{\Lambda} 
\newcommand{\ep}{\epsilon}
\newcommand{\sig}{\sigma} 
\newcommand{\om}{\omega}
\newcommand{\Om}{\Omega}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\MA}{\mathcal{A}}
\newcommand{\MC}{\mathcal{C}}
\newcommand{\MB}{\mathcal{B}}
\newcommand{\MF}{\mathcal{F}}
\newcommand{\MG}{\mathcal{G}}
\newcommand{\ML}{\mathcal{L}}
\newcommand{\MN}{\mathcal{N}}
\newcommand{\MS}{\mathcal{S}}
\newcommand{\MP}{\mathcal{P}}
\newcommand{\ME}{\mathcal{E}}
\newcommand{\MT}{\mathcal{T}}
\newcommand{\MM}{\mathcal{M}}

\newcommand{\z}[1]{Let ${#1} \in \MM_{m,n}$}

\newcommand{\RG}{[0,\infty]}
\newcommand{\Rg}{[0,\infty)}
\newcommand{\limfn}{\liminf \limits_{n \rightarrow \infty}}
\newcommand{\limpn}{\limsup \limits_{n \rightarrow \infty}}
\newcommand{\limn}{\lim \limits_{n \rightarrow \infty}}
\newcommand{\convt}[1]{\xrightarrow{\text{#1}}}
\newcommand{\conv}[1]{\xrightarrow{#1}} 

\newcommand{\Ll}{L^1_{\text{loc}}(\R^n)}
\newcommand{\seq}[1]{(x_{#1})_{#1 \in \N}}

\newcommand{\n}{\Vert}
 
\begin{document}

\title{Linear Model Notes}
\author[James]{Carson James}
\maketitle

\tableofcontents

\section{Matrix Algebra}

\subsection{Column and Null Space}

\begin{ex}
Let $X \in \MM_{m,n}$. Then $\MN(X) = \MN(X^TX)$.
\end{ex}

\begin{proof}
Let $a \in \MN(X)$. Then $Xa = 0$. So $X^TXa = 0$. Thus $a \in \MN(X^TX)$. Conversely, suppose that $a \in \MN(X^TX)$. Then $X^TXa = 0$. So 
\begin{align*}
0 
&= a^TX^TXa \\
&= (Xa)^T(Xa) \\
&= \n Xa \n^2 
\end{align*}
Hence $Xa = 0$ and $a \in \MN(X)$.
\end{proof}

\begin{ex}
Let $X \in \MM_{m,n}$. Then $\MC(X^T) = \MC(X^TX)$.
\end{ex}

\begin{proof}
\begin{align*}
\MC(X^T) 
&= \MN(X)^{\perp} \\
&= \MN(X^TX)^{\perp} \\
&= \MC(X^T X)
\end{align*}
\end{proof}

\begin{ex}
\z{X}. If $X^TX = 0$, then $X = 0$.
\end{ex}

\begin{proof}
Suppose that $X^TX = 0$. Then 
\begin{align*}
rank(X^T)
&= \dim \MC(X^T) \\
&= \dim \MC(X^TX) \\
&= rank(X^TX) \\
&= 0
\end{align*}
So $X^T = X = 0$.
\end{proof}

\begin{ex}
Let $X \in \MM_{m,n}$ and $A,B \in \MM_{n,p}$. Then $X^TXA = X^TXB$ iff $XA = XB$. 
\end{ex}

\begin{proof}
Clearly if $XA = XB$, then $X^TXA = X^TXB$. Conversely, suppose that $X^TXA = X^TXB$. Then $X^TX(A-B) = 0$. So for  each $i =1, \cdots, p$, $X^TX(A-B)e_i = 0$. Thus for each $i=1, \cdots, p$ $X(A-B)e_i \in \MN(X^T) \cap \MC(X) = \{0\}$. Hence $X(A-B) = 0$ and $XA = XB$. 
\end{proof}

\subsection{Generalized Inverses}

\begin{defn}
Let $A \in \MM_{m,n}$ and $G \in \MM_{n,m}$. Then $G$ is said to be a \textbf{generalized inverse} of $A$ if $AGA = A$. 
\end{defn}

\begin{thm}
Let $A \in \MM_{m,n}$. Suppose that $rank(A) = r$. Then there exists $P \in \MM_{m,m}, Q \in \MM_{n,n}, C \in \MM_{r,r}$ such that $P,Q,C$ are non-singular, $rank(C) = r$ and 
\[
A = P
\begin{pmatrix}
C & 0 \\
0 & 0
\end{pmatrix}
Q
\]
\end{thm}

\begin{ex}
Let 
\[
A = P
\begin{pmatrix}
C & 0 \\
0 & 0
\end{pmatrix}
Q
\]
as in the previous theorem and $D \in \MM_{r,m-r}, E \in \MM_{n-r, r}, F \in \MM_{n-r, m-r}$. Put 
\[
G = Q^{-1}
\begin{pmatrix}
C^{-1} & D \\
E & F
\end{pmatrix}
P^{-1}
\]

Then $G$ is a generalized inverse of $A$.
 
\end{ex}

\begin{proof}\
\begin{align*}
AGA 
&= \bigg[P
\begin{pmatrix}
C &0 \\
0 & 0
\end{pmatrix}
Q \bigg]\bigg[Q^{-1}
\begin{pmatrix}
C ^{-1}&D \\
E & F
\end{pmatrix}
P^{-1} \bigg] \bigg[P
\begin{pmatrix}
C &0 \\
0 & 0
\end{pmatrix}
Q\bigg] \\
&= P
\begin{pmatrix}
C &0 \\
0 & 0
\end{pmatrix}
\begin{pmatrix}
C ^{-1}&D \\
E & F
\end{pmatrix}
\begin{pmatrix}
C &0 \\
0 & 0
\end{pmatrix}
Q\\
&= P 
\begin{pmatrix}
I &CD \\
0 & 0
\end{pmatrix}
\begin{pmatrix}
C &0 \\
0 & 0
\end{pmatrix}
Q \\
&= P
\begin{pmatrix}
C &0 \\
0 & 0
\end{pmatrix}
Q \\
&= A
\end{align*}
\end{proof}


\begin{note}
The previous exercise and theorem guarantee the existence of a generealized inverse for all matrices.
We will denote a generalized inverse of $A$ by $A^-$.
\end{note}

\begin{thm}
Let $A \in \MM_{m,n}$. Suppose that $rank(A) = r$. Let $P \in \MM_{mm}$, $Q \in \MM_{n,n}$ permutation matrices and $C \in \MM_{r,r}$. Suppose that $rank(C) = r$ and $PAQ = \begin{pmatrix}
C &D \\
E &F
\end{pmatrix}$ Then $Q\begin{pmatrix}
C^{-1} &0 \\
0 &0
\end{pmatrix}P$ is a generalized inverse of $A$. 
\end{thm}

\begin{ex}
\z{X}. Then $(X^T)^- = (X^-)^T$.
\end{ex}

\begin{proof}
\begin{align*}
X^T(X^-)^TX^T
&= (X X^- X)^T\\
&= X^T
\end{align*}
\end{proof}

\begin{ex}
\z{X}. Then $\MC(XX^-) = \MC(X)$. 
\end{ex}

\begin{proof}
Clearly $\MC(XX^-) \subset \MC(X)$. Let $b \in \MC(X)$. Then there exists $a \in \R^n$ such that $Xa = b$. Then 
\begin{align*}
XX^-b 
&= XX^-Xa \\
&= Xa \\
&= b
\end{align*}

So $b \in \MC(XX^-)$. Thus $\MC(X) \subset \MC(XX^-)$ and $\MC(X) = \MC(XX^-)$
\end{proof}

\begin{ex}
\z{X}. Then $\MN(X) = \MN(X^-X)$
\end{ex}

\begin{proof}
From the previous exercise, we have that 
\begin{align*}
\MN(X) 
&= \MC(X^T)^{\perp} \\
&= \MC(X^T(X^T)^-)^{\perp} \\
&= \MC(X^T(X^-)^T)^{\perp} \\
&= \MC((X^-X)^T)^{\perp} \\
&= \MN(X^-X)
\end{align*}
\end{proof}
\vspace{2mm}

\begin{ex}
\z{X}. Then $X^- = (X^TX)^-X^T$. 
\end{ex}

\begin{proof}
By definition, $X^TX (X^TX)^- X^TX = X^TX$. A previous exercise implies that $X(X^TX)^-X^TX = X$. Thus $X^- = (X^TX)^-X^T$.
\end{proof}

\begin{ex}
\z{X}. Then $(X^T)^- = X(X^TX)^-$. 
\end{ex}

\begin{proof}
The previous exercise tells us that $X^- = (X^TX)^-X^T$. Transposing both sides, we obtain $(X^T)^- = X(X^TX)^-$.
\end{proof}

\subsection{Projections}

\begin{defn}
Let $A \in \MM_{m,m}$. Then $X$ is said to be \textbf{idempotent} if $A^2 = A$.
\end{defn}

\begin{ex}
Let $X \in \MM_{m.n}$. Then $XX^-$ and $X^-X$ are idempotent
\end{ex}

\begin{proof}
\begin{align*}
(XX^-)(XX^-) 
&= (XX^-X)X^- \\
&= XX^-\\
\end{align*} The case is similar for $X^-X$.
\end{proof}

\begin{ex}
Let $A \in \MM_{m.m}$. If $X$ is idempotent, then $I-A$ is idempotent.
\end{ex}

\begin{proof} 
Suppose that $A$ is idempotent. Then
\begin{align*}
(I-A)(I-A) 
&= I^2 -IA -AI + A^2 \\
&= I -2A +A \\
&= I -A
\end{align*}
\end{proof}

\begin{thm}
Let $A \in \MM_{m,m}$. If $A$ is idempotent, then $rank(A) = tr(A)$.
\end{thm}

\begin{defn}
Let $P \in \MM_{m,m}$ and $S \subset \R^m$ a subspace. Then $P$ is said to be a \textbf{projection matrix} onto $S$ if 
\begin{enumerate}
\item $P$ is idempotent
\item $\MC(X) \subset S$ 
\item for each $x \in S$, $Px = x$
\end{enumerate} 
\end{defn}

\begin{note}
In the previous definition, (2) and (3) imply that $\MC(X) = S$, so to say that $X$ projects ``onto" S is accurate.
\end{note}

\begin{ex}
Let $S \subset \R^m$ and $P,Q$ projection matrices onto $S$. Then $PQ = Q$. 
\end{ex}

\begin{proof}
Let $x \in \R^m$. Then $Qx \in \MC(Q) = S$. So $PQx = Qx$. Thus $PQ = Q$.
\end{proof}

\begin{ex}
\z{X}. Then $XX^-$ is a projection onto $\MC(X)$.
\end{ex}

\begin{proof}
A previous exercises tells us that $XX^-$ is idempotent. Another previous exercise tells us that $\MC(XX^-) = \MC(X)$. Let $b \in \MC(X)$. Then there exists $a \in \R^n$ such that $Xa =b$. So 
\begin{align*}
XX^-b
&= XX^- Xa\\ 
&= Xa\\
&= b
\end{align*}
\end{proof}

\begin{ex}
\z{X}. Then $I-X^-X$ is a projection onto $\MN(X)$
\end{ex}

\begin{proof}
Since $X^-X$ is idempotent, so is $I-X^-X$. Let $b \in \MC(I-X^-X)$. Then there exists $a \in \R^n$ such that $(I-X^-X)a = b$. Then 
\begin{align*}
Xb 
&= X(I-X^-X)a \\
&= (X-XX^-X)a \\
&= (X-X)a \\
&= 0a \\ 
&= 0
\end{align*}
So $\MC(I-X^-X) \subset \MN(X)$. Let $a \in \MN(X)$. Then $Xa =0$ and 
\begin{align*}
(I-X^-X)a 
&= a - X^-Xa \\
&= a
\end{align*} 
So for each $a \in \MN(X)$, $(I-X^-X)a = a$.
\end{proof}

\begin{ex}
Let $S \subset \R^m$ be a subspace and $P \in \MM_{m,m}$ be a symmetric projection matrix onto $S$. Then $P$ is unique. 
\end{ex}

\begin{proof}
Let $Q \in \MM_{m,m}$ be a symmetric projection matrix onto $S$. Then 
\begin{align*}
(P-Q)^T(P-Q) 
&= P^TP - P^TQ - Q^TP + Q^TQ \\
&= P^2 - PQ - QP +Q^2 \\
&= P - Q - P + Q \\
&= 0
\end{align*}
Thus $P-Q = 0$ and $P =Q$.
\end{proof}

\begin{defn}
\z{X}. We define $P_X$ by $$P_X = X (X^TX)^-X^T$$
\end{defn}

\begin{ex}
\z{X}. Then $P_X$ is well defined. That is, independent of the choice of $(X^TX)^-$.
\end{ex}

\begin{proof}
Suppose that $G, H$ are generalized inverses of $X^TX$. By definition, we have 
\begin{align*}
X^TXGX^TX = X^TXHX^TX 
& \Rightarrow XGX^TX = XHX^TX \\
& \Rightarrow X^TXG^TX^T = X^TXHX^T \\
& \Rightarrow XG^TX^T = XHX^T \\
& \Rightarrow XGX^T = XHX^T = P_X
\end{align*}
\end{proof}

\begin{note}
Recall that $X^- = (X^TX)^-X^T$. So that $P_X = XX^-$ is indeed a projection onto $\MC(X)$. Recall that $[(X^TX)^-]^T$ is a gen. inv. of $(X^TX)^T = (X^TX)$. Hence $P_X^T = X[(X^TX)^-]^TX^T = P_X$. Since $P_X$ is symmetric, it is the unique symmetric projection onto $\MC(X)$. 
\end{note}

\begin{note}
Recall that $(X^T)^- = X(X^TX)^-$. So that $P_X = (X^T)^-X^T$. A previous exercises tells us that $I-P_X$ is a projection on $\MN(X^T)$. Since $I-P_X$ is symmetric, it is the unique symmetric projection onto $\MN(X^T)$. 
\end{note}

\subsection{Solving Linear Equations}

\begin{defn}
Let $A \in \MM_{m,n}$ and $b \in \R^m$. Then the system $Ax=b$ is said to be \textbf{consistent} if $b \in \MC(A)$.
\end{defn}

\begin{ex}
Let $A \in \MM_{m,n}$ and $G \in \MM_{n,m}$. Then $G = A^-$ iff for each $b \in \MC(A)$, $Gb$ solves $Ax = b$. 
\end{ex}

\begin{proof}
Suppose that $G = A^-$. Let $b \in \MC(A)$. Then there exists $x^* \in \R^n$ such that $Ax^* = b$. So
\begin{align*}
A(Gb)
&= AG(Ax^*) \\
&= (AGA)x^* \\
&= Ax^* \\
&= b
\end{align*}
So $Gb$ solves $Ax =b$. Conversely, Suppose that for each $b \in \MC(A)$, $Gb$ solves $Ax = b$. Let $z \in \R^n$. So $Az \in \MC(A)$. Then  
\begin{align*}
(AGA)z 
&= A[G(Az)] \\
&= Az
\end{align*}
Since for each $z \in \R^n AGAz = Az$, $AGA = A$ and $G = A^-$.
\end{proof}
\vspace{2mm}

\begin{ex}
Let $b \in \MC(A)$. Then $$\{x \in \R^n: Ax = b\} = \{A^-b+(I-A^-A)z: z \in \R^n \}$$.
\end{ex}

\begin{proof}
Let $x \in \{A^-b+(I-A^-A)z: z \in \R^n \}$. Then there exists $z \in \R^n$ such that $x = A^-b+(I-A^-A)z$. Since $(I-A^-A)$ is a projection onto $\MN(A)$, 
\begin{align*}
Ax
&= AA^-b \\
&= b
\end{align*}
So $x \in \{x \in \R^n: Ax = b\}$. Conversely, let $x \in \{x \in \R^n: Ax = b\}$. Then 
\begin{align*}
x 
&= A^-(Ax) + (x - A^-Ax) \\
&= A^-(b) + (I-A^-A)x \\
& \in \{A^-b+(I-A^-A)z: z \in \R^n \}
\end{align*}
\end{proof}

\subsection{Moore-Penrose Pseudoinverse}

\begin{thm} (Singular Value Decomposition): \\
Let $A \in \MM_{m,n}$. Suppose that $rank(A) = r$. Then there exist $U \in \MM_{m,m} V \in \MM_{n,n}$, and $D_0 \in \MM_{r,r}$ such that
\begin{enumerate}
\item $A = 
U
\begin{pmatrix}
D_0 & 0 \\
0 & 0
\end{pmatrix}
V^T
$
\item  
$U^T
U = I$
\item $V^T
V = I$ 
\item $D_0 = diagonal(d_1, d_2, \cdots, d_r)$ with $d_1 \geq d_2 \geq \cdots \geq d_r >0$
\end{enumerate}

\end{thm}

\begin{note}
Put $D = 
\begin{pmatrix}
D_0 & 0 \\
0 & 0
\end{pmatrix} \in \MM_{m,n}$ 
\begin{enumerate}
\item Since $D_0$ is symmetric, $D^T = 
\begin{pmatrix}
D_0 & 0 \\
0 & 0
\end{pmatrix}  \in \MM_{n,m}$
\item Since $D_0$ is diagonal, $D_0^{-1}$ is also diagonal and symmetric
\end{enumerate}
\end{note}

\begin{defn}
Let $A \in \MM_{m,m}$ and $A^+ \in \MM_{n,m}$. Then $A^+$ is said to be a  \textbf{Moore-Penrose pseudoinverse} of $A$ if 
\begin{enumerate}
\item $AA^+A = A$
\item $A^+AA^+ = A^+$
\item $AA^+$ is symmetric
\item $A^+A$ is symmetric
\end{enumerate} 
\end{defn}

\begin{note}
We have that $P_X = XX^+ = X(X^TX)^-X^T$. 
\end{note}

\begin{ex}
Let $A \in \MM_{m,n}$ and $S,T \in \MM_{n,m}$. If $S$ and $T$ are m-p pseudoinverses of $A$, then $S=T$. 
\end{ex}

\begin{proof}
Suppose that $S,T$ satisfy properties (1)-(4). Then 
\begin{align*}
S
&= SAS \\
&= (SA)^TS \\
&= A^TS^TS \\
&= (ATA)^TS^TS \\
&= A^TT^TA^TS^TS \\
&= (TA)^T(SA)^TS \\
&= (TA)(SA)S \\
&= TA(SAS) \\
&= TAS
\end{align*}
and 
\begin{align*}
T 
&= TAT \\
&= T(AT)^T \\
&= TT^TA^T \\
&= TT^T(ASA)^T \\
&= TT^TA^TS^TA^T \\
&= T(AT)^T (AS)^T \\
&= T(AT) (AS) \\
&= (TAT)AS \\
&= TSA 
\end{align*} 

So $S=T$
\end{proof}

\begin{ex}
Let $A \in \MM_{m, n}$ have singular value decomposition $A = UDV^T = U
\begin{pmatrix}
D_0 & 0 \\
0 & 0
\end{pmatrix}
V^T$. Define $D^+ = \begin{pmatrix}
D_0^{-1} & 0 \\
0 & 0
\end{pmatrix} \in \MM_{n,m}$. Then $D^+$ is the m-p pseudoinverse of $D$.
\end{ex}

\begin{proof}\
\begin{enumerate}
\item 
\begin{align*}
DD^+D 
&= \begin{pmatrix}
D_0 & 0 \\
0 & 0
\end{pmatrix} 
\begin{pmatrix}
D_0^{-1} & 0 \\
0 & 0
\end{pmatrix} 
\begin{pmatrix}
D_0 & 0 \\
0 & 0
\end{pmatrix} \\
&= \begin{pmatrix}
I & 0 \\
0 & 0
\end{pmatrix}
\begin{pmatrix}
D_0 & 0 \\
0 & 0
\end{pmatrix} \\ 
&= \begin{pmatrix}
D_0 & 0 \\
0 & 0
\end{pmatrix} \\
&= D
\end{align*}
\item Similar to (1).
\item 
\begin{align*}
(DD^{+})^T
&=\begin{pmatrix}
I & 0 \\
0 & 0
\end{pmatrix}^T \\
&= \begin{pmatrix}
I & 0 \\
0 & 0
\end{pmatrix} \\
&= DD^{+}
\end{align*}
\item Similar to (3).
\end{enumerate}

\end{proof}

\begin{ex}
Let $A \in \MM_{m, n}$ have singular value decomposition $A = UDV^T$. So $A^T \in \MM_{n,m}$ has singular value decomposition $A^T = VD^TU^T$.
Then $(D^T)^+ = (D^+)^T$
\end{ex}

\begin{proof}
Since $D^T = \begin{pmatrix}
D_0 & 0 \\
0 & 0
\end{pmatrix} \in \MM_{n,m}$, we have that $(D^T)^+ = 
\begin{pmatrix}
D_0^{-1} & 0 \\
0 & 0
\end{pmatrix}
= (D^+)^T$
\end{proof}

\begin{ex}
Let $A \in \MM_{m, n}$ have singular value decomposition $A = UDV^T$. Define $A^+ = VD^+U^T$. Then $A^+$ is the m-p pseudoinverse of $A$.
\end{ex}

\begin{proof}
\begin{enumerate}
\item 
\begin{align*}
AA^+A 
&= (UDV^T)(VD^+U^T)(UDV^T) \\
&= UDD^+DV^T \\
&= UDV^T \\
&= A
\end{align*}
\item Similar to (1)
\item 
\begin{align*}
(AA^+)^T
&= \big[(UDV^T)(VD^+U^T)\big]^T\\
&= (UDD^+U^T)^T \\
&= U(DD^+)^TU^T \\
&= U DD^+ U^T \\
&= (UDV^T)(VD^+U^T) \\
&= AA^+
\end{align*}
\item Similar to (3).
\end{enumerate}
\end{proof}

\begin{ex}
Let $A \in \MM_{m,n}$ have singular value decomposition $A = UDV^T$. Then $(A^T)^+ = (A^+)^T$.
\end{ex}

\begin{proof} \
\begin{align*}
(A^T)^+ 
&= \big[(UDV^T)^T\big]^+ \\
&= (VD^TU^T)^+ \\
&= U(D^T)^+V^T \\
&= U(D^+)^TV^T \\
&= (VD^+U^T)^T \\
&= (A^+)^T
\end{align*}
\end{proof}
\vspace{3mm}


\begin{ex} Let $A \in \MM_{m,n}$. Then there exists a unique matrix $A^+ \in \MM_{n,m}$ such that $A^+$ is the m-p pseudoinverse of $A$. 
\end{ex}

\begin{proof}
The existence of and uniqueness of $A^+$ are shown in the previous exercises. 
\end{proof}

\begin{ex}
Let $A \in \MM_{m,m}$. Then $(A^+)^+ = A$.
\end{ex}

\begin{proof}
We observe that $A$ satisfies properties $(1)-(4)$ for $A^+$. By uniqueness, $(A^+)^+ = A$.
\end{proof}

\begin{ex}
Let $A \in \MM_{m,n}$ and $b \in \MC(A)$. Pur $S = \{x \in \R^n: Ax= b\}$. Then $$\n A^+b \n = \min_{x \in S}\n x \n$$.
\end{ex}

\begin{proof} Let $x \in S$. A previous exercise tells us that there exists $z \in \R^n$ such that $x = A^+b + (I-A^+A)z$. Then 
\begin{align*} 
\n x \n^2
&= \n A^+b + (I-A^+A)z \n^2 \\
&= (A^+b + (I-A^+A)z)^T(A^+b + (I-A^+A)z) \\
&= \n A^+b \n^2 - 2z^T(I-A^+A)^T(A^+b) + \n (I-A^+A)z \n^2 \\
&= \n A^+b \n^2 - 2z^T(I-A^+A)A^+b + \n (I-A^+A)z \n^2 \\
&= \n A^+b \n^2 + \n (I-A^+A)z \n^2 \\
& \geq \n A^+b \n^2
\end{align*}
\end{proof}

\subsection{Differentiation}

\begin{defn}
Let $Q:\R^n \rightarrow \R$ given by $b \mapsto Q(b)$. Suppose that $Q \in C^1(\R^n)$.  We define 
\[ 
\pdv{Q}{b}= 
\begin{pmatrix} 
\pdv{Q}{b_1}  \\
\vdots\\
\pdv{Q}{b_n}
\end{pmatrix}  
\]
\end{defn}

\begin{ex}
Let $a, b  \in \R_n$ and $A \in \MM_{n, n}$. Then \vspace{2mm}
\begin{enumerate}
\Item $$\pdv{a^Tb}{b} = a$$ \vspace{2mm}
\Item $$\pdv{b^T A b}{b} = (A+A^T)b$$
\end{enumerate} 
\end{ex}

\begin{proof}\
\begin{enumerate}
\Item Since $$a^Tb = \sum_{i=1}^n a_ib_i$$ We have that $$\pdv{a^Tb}{b_i} = a_i$$ and therefore $$\frac{\partial a^Tb}{\partial b} = a$$ \vspace{3mm}
\Item Since 
\begin{align*}
b^T A b 
&= \sum_{i = 1}^n b_i \sum_{j=1}^n A_{i,j}b_j \\
&= \sum_{i = 1}^n \sum_{j=1}^n b_iA_{i,j}b_j \\
\end{align*}
The terms containing $b_i$ are $$A_{i,i}b_i^2 + \sum_{\substack{ j=1 \\ j \neq i}}^n (A_{i,j} + A_{j,i})b_ib_j$$
This implies that 
\begin{align*}
\frac{\partial b^TAb}{\partial b_i} 
&= 2A_{i,i}b_i + \sum_{\substack{j=1 \\ j\neq i}}^n (A_{i,j}+A_{j,i})b_j\\
&= \sum_{j=1}^n (A_{i,j}+A^T_{i,j})b_j\\
&= [(A+A^T)b]_i
\end{align*}
So $$\pdv{b^T A b}{b} = (A+A^T)b$$
\end{enumerate}
\end{proof} 

\section{The Linear Model}
\subsection{Model Description}
\begin{defn}
Given $y \in \R_m$ a vector of observed responses to the matrix $X \in \MM_{m,n}$ of observed inputs, we will consider the model $$y = Xb +e$$ where $b \in \R_n$ is a vector of unknown parameters and $e \in \R^m$ is a random vector of unobserved errors with zero mean. 
\end{defn}

\begin{defn}
For a parameter vector $b \in \R^n$, we have that  $e = y-Xb$. For this reason, $e$ is called the \textbf{residual vector} or simply the ``residuals''.
\end{defn}

\begin{note}
The goal will be to find a parameter vector $b \in \R^n$ that makes the causes the residuals to be as small as possible.  
\end{note}

\subsection{Least Squares Optimization}

\begin{defn}
We define the \textbf{cost function}, $Q: \R^n \rightarrow \R$ by 
\begin{align*}
Q(b) 
&= \n y - Xb \n ^2 \\
&= (y - Xb )^T(y - Xb )
\end{align*}
\end{defn}

\begin{defn}
Let $b \in \R^n$. Then $b$ is said to be a \textbf{least squares solution} for the model if $$Q(b) = \inf_{c \in R^{n}} Q(c)$$ 
\end{defn}

\begin{ex}
If $b$ is a least squares solution for the model, then $X^TXb = X^Ty$.
\end{ex}

\begin{proof}
Suppose that $b$ is a least squares solution for the model, then $Q$ has a local minimum at $b$. Since $Q$ is convex in $b$, this global minimum is also a local minimum. Thus $$\frac{\partial Q}{\partial b}(b) = 0$$ By definition, 
\begin{align*}
Q(b) 
&= y^Ty -y^TXb - b^TX^Ty + b^TX^TXb \\
&= y^Ty -2y^TXb + b^TX^TXb \\ 
\end{align*}
Thus 
\begin{align*}
0
&= \frac{\partial Q}{\partial b}(b) \\
&= -2X^Ty + 2X^TXb
\end{align*}
Hence $X^TXb = X^Ty$.
\end{proof}

\begin{defn}
For $y \in \R^m$ and $X \in \MM_{m,n}$, we define the \textbf{normal equation} to be $$X^TXb = X^Ty$$
\end{defn}

\begin{ex}
The normal equation is consistent.
\end{ex}

\begin{proof}
We have that $X^Ty \in \MC(X^T) = \MC(X^TX)$. 
\end{proof}

\begin{ex}
Let $b \in \R^n$. Then $b$ is a least squares solution for the model iff $b$ satisfies the normal equation.
\end{ex}

\begin{proof}
The previous exercises tells us that if $b$ is a least squares solution for the model, then $b$ satisfies the normal equation. Conversely, suppose that $b$ satisfies the normal equation. Then 
\begin{align*}
Q(c) 
&= (y - Xc )^T(y - Xc ) \\
&= (y - Xb +Xb -Xc )^T(y - Xb +Xb -Xc ) \\
&= (y - Xb)^T (y - Xb) - (y - Xb)^T(X(b-c)) - (b-c)^TX^T(y - Xb) + (b-c)^TX^T(X(b-c)) \\
&= Q(b) -2(b-c)^TX^T(y-Xb) + \n X(b-c) \n^2 \\
&= Q(b)+ \n X(b-c) \n^2 
\end{align*}
Thus $b$ minimizes $Q$.
\end{proof}

\begin{ex}
Let $b \in \R^n$ be a least squares solution for the model. Then $\n y \n^2 = \n Xb \n^2 + \n e \n^2$
\end{ex}

\begin{proof}
Since $b$ satisfies the normal equation, we have that $X^T(y - Xb) = 0$. Thus 
\begin{align*}
Xb \cdot e
&= b^TX^Te \\
&= b^TX^T(y - Xb) \\
&= b^T0 \\
&=0
\end{align*}
So $Xb$ and $e$ are orthogonal. Therefore 
\begin{align*}
\n y\n^2 
&= \n Xb+ e\n^2 \\
&= \n Xb \n^2 + \n e \n^2
\end{align*}
\end{proof}




\end{document}
